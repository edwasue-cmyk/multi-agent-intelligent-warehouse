# =============================================================================
# Warehouse Operational Assistant - Environment Configuration
# =============================================================================
# 
# Copy this file to .env and update with your actual values:
#   cp .env.example .env
#   nano .env  # or your preferred editor
#
# For Docker Compose deployments, place .env in deploy/compose/ directory
# =============================================================================

# =============================================================================
# ENVIRONMENT
# =============================================================================
# Set to 'production' for production deployments, 'development' for local dev
ENVIRONMENT=development

# =============================================================================
# DATABASE CONFIGURATION (PostgreSQL/TimescaleDB)
# =============================================================================
# Database connection settings
POSTGRES_USER=warehouse
POSTGRES_PASSWORD=changeme  # ⚠️ CHANGE IN PRODUCTION!
POSTGRES_DB=warehouse
DB_HOST=localhost
DB_PORT=5435

# Alternative database URL format (overrides individual settings above)
# DATABASE_URL=postgresql://warehouse:changeme@localhost:5435/warehouse

# =============================================================================
# SECURITY
# =============================================================================
# JWT Secret Key - REQUIRED for production, optional for development
# Generate a strong random key: openssl rand -hex 32
# Minimum 32 characters recommended
JWT_SECRET_KEY=your-strong-random-secret-minimum-32-characters-change-this-in-production

# Admin user default password (change in production!)
DEFAULT_ADMIN_PASSWORD=changeme

# =============================================================================
# REDIS CONFIGURATION
# =============================================================================
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=  # Leave empty for development
REDIS_DB=0

# =============================================================================
# VECTOR DATABASE (Milvus)
# =============================================================================
MILVUS_HOST=localhost
MILVUS_PORT=19530
MILVUS_USER=root
MILVUS_PASSWORD=Milvus

# GPU Acceleration for Milvus
MILVUS_USE_GPU=true
MILVUS_GPU_DEVICE_ID=0
CUDA_VISIBLE_DEVICES=0
MILVUS_INDEX_TYPE=GPU_CAGRA
MILVUS_COLLECTION_NAME=warehouse_docs_gpu

# =============================================================================
# MESSAGE QUEUE (Kafka)
# =============================================================================
KAFKA_BOOTSTRAP_SERVERS=localhost:9092
# Alternative: KAFKA_BROKER=kafka:9092

# =============================================================================
# NVIDIA NIM LLM CONFIGURATION
# =============================================================================
# 
# IMPORTANT: Different models use different endpoints!
# 
# For the 49B model (llama-3.3-nemotron-super-49b-v1):
#   - Use: https://api.brev.dev/v1
#   - This is the correct endpoint for the 49B model
#
# For other NVIDIA NIM models:
#   - Use: https://integrate.api.nvidia.com/v1
#   - This is the standard NVIDIA NIM endpoint
#
# For self-hosted NIM instances:
#   - Use your own endpoint URL (e.g., http://localhost:8000/v1 or https://your-nim-instance.com/v1)
#   - Ensure your NIM instance is accessible and properly configured
#
# Your NVIDIA API key (same key works for both endpoints)
NVIDIA_API_KEY=your-nvidia-api-key-here

# LLM Service Endpoint
# For 49B model: https://api.brev.dev/v1
# For other NIMs: https://integrate.api.nvidia.com/v1
# For self-hosted: http://your-nim-host:port/v1
LLM_NIM_URL=https://api.brev.dev/v1

# LLM Model Identifier
# Example for 49B model:
LLM_MODEL=nvcf:nvidia/llama-3.3-nemotron-super-49b-v1:dep-36ZiLbQIG2ZzK7gIIC5yh1E6lGk

# LLM Generation Parameters
LLM_TEMPERATURE=0.1
LLM_MAX_TOKENS=2000
LLM_TOP_P=1.0
LLM_FREQUENCY_PENALTY=0.0
LLM_PRESENCE_PENALTY=0.0
LLM_CLIENT_TIMEOUT=120  # Timeout in seconds

# LLM Caching
LLM_CACHE_ENABLED=true
LLM_CACHE_TTL_SECONDS=300  # Cache TTL in seconds (5 minutes)

# =============================================================================
# EMBEDDING SERVICE CONFIGURATION
# =============================================================================
# Embedding service endpoint (typically uses NVIDIA endpoint)
EMBEDDING_NIM_URL=https://integrate.api.nvidia.com/v1
# Embedding API key (usually same as NVIDIA_API_KEY)
# EMBEDDING_API_KEY=your-embedding-api-key  # Defaults to NVIDIA_API_KEY if not set

# =============================================================================
# CORS CONFIGURATION
# =============================================================================
# Allowed origins for CORS (comma-separated)
# Add your frontend URLs here
CORS_ORIGINS=http://localhost:3001,http://localhost:3000,http://127.0.0.1:3001,http://127.0.0.1:3000

# =============================================================================
# UPLOAD & REQUEST LIMITS
# =============================================================================
# Maximum request size in bytes (default: 10MB)
MAX_REQUEST_SIZE=10485760

# Maximum upload size in bytes (default: 50MB)
MAX_UPLOAD_SIZE=52428800

# =============================================================================
# NeMo Guardrails Configuration
# =============================================================================
# RAIL_API_KEY=your_nvidia_ngc_api_key_here

# =============================================================================
# Document Extraction Agent - NVIDIA NeMo API Keys
# =============================================================================
# NEMO_RETRIEVER_API_KEY=your_nvidia_ngc_api_key_here
# NEMO_OCR_API_KEY=your_nvidia_ngc_api_key_here
# NEMO_PARSE_API_KEY=your_nvidia_ngc_api_key_here
# LLAMA_NANO_VL_API_KEY=your_nvidia_ngc_api_key_here
# LLAMA_70B_API_KEY=your_nvidia_ngc_api_key_here

# =============================================================================
# EXTERNAL SERVICE INTEGRATIONS
# =============================================================================
# WMS_API_KEY=your-wms-api-key
# ERP_API_KEY=your-erp-api-key

# =============================================================================
# NOTES FOR DEVELOPERS
# =============================================================================
#
# 1. LLM Endpoint Configuration:
#    - The 49B model REQUIRES https://api.brev.dev/v1
#    - Other NIM models use https://integrate.api.nvidia.com/v1
#    - Both endpoints use the same NVIDIA_API_KEY
#    - You can deploy NIMs on your own instances and consume them via endpoint
#      (e.g., http://localhost:8000/v1 or https://your-nim-instance.com/v1)
#    - For self-hosted NIMs, ensure the endpoint is accessible and properly configured
#
# 2. Security:
#    - NEVER commit .env files to version control
#    - Change all default passwords in production
#    - Use strong, unique JWT_SECRET_KEY in production
#    - JWT_SECRET_KEY is REQUIRED in production (app will fail to start without it)
#
# 3. Database:
#    - Default port 5435 is used to avoid conflicts with standard PostgreSQL (5432)
#    - Ensure Docker containers are running before starting the backend
#
# 4. Testing:
#    - View logs in real-time: ./scripts/view_logs.sh
#    - Restart backend: ./restart_backend.sh
#    - Check health: curl http://localhost:8001/api/v1/health
#
# 5. Getting NVIDIA API Keys:
#    - Sign up at: https://build.nvidia.com/
#    - Get your API key from the NVIDIA dashboard
#    - The same key works for both brev.dev and integrate.api.nvidia.com endpoints
#
# =============================================================================
